{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyrL1t9ewlX_",
        "outputId": "234966dc-96d9-4ba6-d93b-288016561caf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Akpp3i__zcfP"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Define the zip file path and extraction path\n",
        "zip_path = \"/content/drive/My Drive/brain_tumor_dataset/binary_class.zip\"\n",
        "extract_path = \"/content/binary_class\"\n",
        "\n",
        "# Unzip the file\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rxS1T-9z5sq",
        "outputId": "76611ee0-9a5e-477e-a281-02f156956f09"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Testing', 'Training']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# List extracted files/folders\n",
        "os.listdir(extract_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjn-h6xM7s_d",
        "outputId": "cafd9649-1781-4e06-f7e5-7ee303bf9b0f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import os\n",
        "os.path.exists(\"/content/binary_class\")  # ➡️ True hona chahiye\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FP-_Ug7bFMXN"
      },
      "outputs": [],
      "source": [
        "# Write dependencies to requirements.txt\n",
        "with open(\"requirements.txt\", \"w\") as f:\n",
        "    f.write(\"\"\"\\\n",
        "numpy\n",
        "pandas\n",
        "matplotlib\n",
        "seaborn\n",
        "scikit-learn\n",
        "opencv-python\n",
        "pillow\n",
        "\n",
        "torch\n",
        "torchvision\n",
        "timm\n",
        "\n",
        "catboost\n",
        "optuna\n",
        "\n",
        "shap\n",
        "\n",
        "reportlab\n",
        "\n",
        "streamlit\n",
        "gradio\n",
        "\n",
        "tqdm\n",
        "pathlib\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEoXl5OBGc56",
        "outputId": "a6df56ea-ee80-4779-8d3c-db8776c7f2b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (0.13.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (1.6.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (4.12.0.88)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (11.3.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 9)) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 10)) (0.21.0+cu124)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 11)) (1.0.19)\n",
            "Requirement already satisfied: catboost in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 13)) (1.2.8)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 14)) (4.4.0)\n",
            "Requirement already satisfied: shap in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 16)) (0.48.0)\n",
            "Requirement already satisfied: reportlab in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 18)) (4.4.3)\n",
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 20)) (1.47.1)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 21)) (5.38.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 23)) (4.67.1)\n",
            "Requirement already satisfied: pathlib in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 24)) (1.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 2)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 2)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 2)) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 3)) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 3)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 3)) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 3)) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 3)) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 3)) (3.2.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirements.txt (line 5)) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirements.txt (line 5)) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirements.txt (line 5)) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->-r requirements.txt (line 9)) (1.3.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm->-r requirements.txt (line 11)) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm->-r requirements.txt (line 11)) (0.34.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm->-r requirements.txt (line 11)) (0.5.3)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost->-r requirements.txt (line 13)) (0.21)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost->-r requirements.txt (line 13)) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost->-r requirements.txt (line 13)) (1.17.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna->-r requirements.txt (line 14)) (1.16.4)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna->-r requirements.txt (line 14)) (6.9.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna->-r requirements.txt (line 14)) (2.0.41)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.11/dist-packages (from shap->-r requirements.txt (line 16)) (0.0.8)\n",
            "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.11/dist-packages (from shap->-r requirements.txt (line 16)) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from shap->-r requirements.txt (line 16)) (3.1.1)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from reportlab->-r requirements.txt (line 18)) (3.4.2)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r requirements.txt (line 20)) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r requirements.txt (line 20)) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r requirements.txt (line 20)) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r requirements.txt (line 20)) (8.2.1)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r requirements.txt (line 20)) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r requirements.txt (line 20)) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r requirements.txt (line 20)) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r requirements.txt (line 20)) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r requirements.txt (line 20)) (0.10.2)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r requirements.txt (line 20)) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r requirements.txt (line 20)) (3.1.45)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r requirements.txt (line 20)) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r requirements.txt (line 20)) (6.4.2)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio->-r requirements.txt (line 21)) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio->-r requirements.txt (line 21)) (4.9.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from gradio->-r requirements.txt (line 21)) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio->-r requirements.txt (line 21)) (0.116.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio->-r requirements.txt (line 21)) (0.6.1)\n",
            "Requirement already satisfied: gradio-client==1.11.0 in /usr/local/lib/python3.11/dist-packages (from gradio->-r requirements.txt (line 21)) (1.11.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio->-r requirements.txt (line 21)) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio->-r requirements.txt (line 21)) (0.28.1)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio->-r requirements.txt (line 21)) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio->-r requirements.txt (line 21)) (3.11.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio->-r requirements.txt (line 21)) (2.11.7)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio->-r requirements.txt (line 21)) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio->-r requirements.txt (line 21)) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio->-r requirements.txt (line 21)) (0.12.5)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio->-r requirements.txt (line 21)) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio->-r requirements.txt (line 21)) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio->-r requirements.txt (line 21)) (0.47.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio->-r requirements.txt (line 21)) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio->-r requirements.txt (line 21)) (0.16.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio->-r requirements.txt (line 21)) (0.35.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.11.0->gradio->-r requirements.txt (line 21)) (15.0.1)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna->-r requirements.txt (line 14)) (1.1.3)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit->-r requirements.txt (line 20)) (4.25.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit->-r requirements.txt (line 20)) (1.48.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio->-r requirements.txt (line 21)) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio->-r requirements.txt (line 21)) (1.3.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit->-r requirements.txt (line 20)) (4.0.12)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0,>=0.24.1->gradio->-r requirements.txt (line 21)) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0,>=0.24.1->gradio->-r requirements.txt (line 21)) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio->-r requirements.txt (line 21)) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm->-r requirements.txt (line 11)) (1.1.5)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.54->shap->-r requirements.txt (line 16)) (0.43.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio->-r requirements.txt (line 21)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio->-r requirements.txt (line 21)) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio->-r requirements.txt (line 21)) (0.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit->-r requirements.txt (line 20)) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna->-r requirements.txt (line 14)) (3.2.3)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio->-r requirements.txt (line 21)) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio->-r requirements.txt (line 21)) (13.9.4)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit->-r requirements.txt (line 20)) (5.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r requirements.txt (line 20)) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r requirements.txt (line 20)) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r requirements.txt (line 20)) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r requirements.txt (line 20)) (0.26.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio->-r requirements.txt (line 21)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio->-r requirements.txt (line 21)) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio->-r requirements.txt (line 21)) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zoCnJVENIVMf"
      },
      "outputs": [],
      "source": [
        "from reportlab.lib.colors import Color\n",
        "\n",
        "# If you originally had:\n",
        "# RGBColor(255, 0, 0)\n",
        "\n",
        "# Replace with:\n",
        "red_color = Color(1.0, 0.0, 0.0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIr75vr9JElr"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import argparse\n",
        "\n",
        "# Only keep arguments you want (ignore Colab/Jupyter args)\n",
        "sys.argv = ['script.py']  # Reset to only your script name\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--mode', choices=['train', 'predict', 'demo', 'streamlit', 'gradio'], required=False, default='train')\n",
        "parser.add_argument('--training_data', type=str, default=None)\n",
        "parser.add_argument('--image', type=str, default=None)\n",
        "parser.add_argument('--output_dir', type=str, default=None)\n",
        "args = parser.parse_args()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "7cb423c139014a098ef3a29e8b0f075b",
            "9300a6ea3bcd4f9dac598133bad5b799",
            "4a1ac1a2f9d54969af0b5b48d0c96b88",
            "b222d729f0c9442ea2030b10918730d2",
            "b61d822665b341abb8d17256844bc269",
            "251f6dbda51f4ee18ba339f63dc56af3",
            "dc019972e2c64dc88b0acc2b80c0fc57",
            "e024dcd0a7e342c7ac9147bc5dbfd72d",
            "07904c0240974f51abda600cab916248",
            "6dfeedc0cf1b4b24a7e9a5d21a5d0132",
            "5f22402fd2214d99b4b77b1d07126f9a",
            "220b3ae94b9d4219bdffd14a6b223f8b",
            "b615d48c938d4da38b27c4f7f8531b79",
            "c3a70517191b4e5b97982faa539e1073",
            "769401a13b3d44cd876865229a8eb015",
            "d897b78520b74505b940ec98e0533e05",
            "ecba93d6e62b49d5a075742fe2d1073b",
            "e16268bee90e4cb7b1cdbbcf7a628644",
            "85e54948d49948bcbb35a9ae6c805052",
            "9ae0953ff23b43b8bf015ccfb65f4174",
            "99ac06a05a9147ad852f774674403f40",
            "bdf84749d1b94b589b1e676aa50c2de2"
          ]
        },
        "id": "16d62888",
        "outputId": "feb4a05b-3ee6-4012-fa5f-41590261fd87"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🧠 Brain Tumor Classifier Demo\n",
            "==================================================\n",
            "Using device: cpu\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7cb423c139014a098ef3a29e8b0f075b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Starting model training pipeline...\n",
            "Loading training data...\n",
            "Loaded 5712 training images\n",
            "🔄 Preprocessing images...\n",
            "Processing 5712 images...\n",
            "Processed 100/5712 images (1.8%)\n",
            "Processed 200/5712 images (3.5%)\n",
            "Processed 300/5712 images (5.3%)\n",
            "Processed 400/5712 images (7.0%)\n",
            "Processed 500/5712 images (8.8%)\n",
            "Processed 600/5712 images (10.5%)\n",
            "Processed 700/5712 images (12.3%)\n",
            "Processed 800/5712 images (14.0%)\n",
            "Processed 900/5712 images (15.8%)\n",
            "Processed 1000/5712 images (17.5%)\n",
            "Processed 1100/5712 images (19.3%)\n",
            "Processed 1200/5712 images (21.0%)\n",
            "Processed 1300/5712 images (22.8%)\n",
            "Processed 1400/5712 images (24.5%)\n",
            "Processed 1500/5712 images (26.3%)\n",
            "Processed 1600/5712 images (28.0%)\n",
            "Processed 1700/5712 images (29.8%)\n",
            "Processed 1800/5712 images (31.5%)\n",
            "Processed 1900/5712 images (33.3%)\n",
            "Processed 2000/5712 images (35.0%)\n",
            "Processed 2100/5712 images (36.8%)\n",
            "Processed 2200/5712 images (38.5%)\n",
            "Processed 2300/5712 images (40.3%)\n",
            "Processed 2400/5712 images (42.0%)\n",
            "Processed 2500/5712 images (43.8%)\n",
            "Processed 2600/5712 images (45.5%)\n",
            "Processed 2700/5712 images (47.3%)\n",
            "Processed 2800/5712 images (49.0%)\n",
            "Processed 2900/5712 images (50.8%)\n",
            "Processed 3000/5712 images (52.5%)\n",
            "Processed 3100/5712 images (54.3%)\n",
            "Processed 3200/5712 images (56.0%)\n",
            "Processed 3300/5712 images (57.8%)\n",
            "Processed 3400/5712 images (59.5%)\n",
            "Processed 3500/5712 images (61.3%)\n",
            "Processed 3600/5712 images (63.0%)\n",
            "Processed 3700/5712 images (64.8%)\n",
            "Processed 3800/5712 images (66.5%)\n",
            "Processed 3900/5712 images (68.3%)\n",
            "Processed 4000/5712 images (70.0%)\n",
            "Processed 4100/5712 images (71.8%)\n",
            "Processed 4200/5712 images (73.5%)\n",
            "Processed 4300/5712 images (75.3%)\n",
            "Processed 4400/5712 images (77.0%)\n",
            "Processed 4500/5712 images (78.8%)\n",
            "Processed 4600/5712 images (80.5%)\n",
            "Processed 4700/5712 images (82.3%)\n",
            "Processed 4800/5712 images (84.0%)\n",
            "Processed 4900/5712 images (85.8%)\n",
            "Processed 5000/5712 images (87.5%)\n",
            "Processed 5100/5712 images (89.3%)\n",
            "Processed 5200/5712 images (91.0%)\n",
            "Processed 5300/5712 images (92.8%)\n",
            "Processed 5400/5712 images (94.5%)\n",
            "Processed 5500/5712 images (96.3%)\n",
            "Processed 5600/5712 images (98.0%)\n",
            "Processed 5700/5712 images (99.8%)\n",
            "Processed 5712/5712 images (100.0%)\n",
            "🎯 Extracting ViT features...\n",
            "Extracting ViT features in 357 batches...\n",
            "Processed batch 1/357\n",
            "Processed batch 2/357\n",
            "Processed batch 3/357\n",
            "Processed batch 4/357\n",
            "Processed batch 5/357\n",
            "Processed batch 6/357\n",
            "Processed batch 7/357\n",
            "Processed batch 8/357\n",
            "Processed batch 9/357\n",
            "Processed batch 10/357\n",
            "Processed batch 11/357\n",
            "Processed batch 12/357\n",
            "Processed batch 13/357\n",
            "Processed batch 14/357\n",
            "Processed batch 15/357\n",
            "Processed batch 16/357\n",
            "Processed batch 17/357\n",
            "Processed batch 18/357\n",
            "Processed batch 19/357\n",
            "Processed batch 20/357\n",
            "Processed batch 21/357\n",
            "Processed batch 22/357\n",
            "Processed batch 23/357\n",
            "Processed batch 24/357\n",
            "Processed batch 25/357\n",
            "Processed batch 26/357\n",
            "Processed batch 27/357\n",
            "Processed batch 28/357\n",
            "Processed batch 29/357\n",
            "Processed batch 30/357\n",
            "Processed batch 31/357\n",
            "Processed batch 32/357\n",
            "Processed batch 33/357\n",
            "Processed batch 34/357\n",
            "Processed batch 35/357\n",
            "Processed batch 36/357\n",
            "Processed batch 37/357\n",
            "Processed batch 38/357\n",
            "Processed batch 39/357\n",
            "Processed batch 40/357\n",
            "Processed batch 41/357\n",
            "Processed batch 42/357\n",
            "Processed batch 43/357\n",
            "Processed batch 44/357\n",
            "Processed batch 45/357\n",
            "Processed batch 46/357\n",
            "Processed batch 47/357\n",
            "Processed batch 48/357\n",
            "Processed batch 49/357\n",
            "Processed batch 50/357\n",
            "Processed batch 51/357\n",
            "Processed batch 52/357\n",
            "Processed batch 53/357\n",
            "Processed batch 54/357\n",
            "Processed batch 55/357\n",
            "Processed batch 56/357\n",
            "Processed batch 57/357\n",
            "Processed batch 58/357\n",
            "Processed batch 59/357\n",
            "Processed batch 60/357\n",
            "Processed batch 61/357\n",
            "Processed batch 62/357\n",
            "Processed batch 63/357\n",
            "Processed batch 64/357\n",
            "Processed batch 65/357\n",
            "Processed batch 66/357\n",
            "Processed batch 67/357\n",
            "Processed batch 68/357\n",
            "Processed batch 69/357\n",
            "Processed batch 70/357\n",
            "Processed batch 71/357\n",
            "Processed batch 72/357\n",
            "Processed batch 73/357\n",
            "Processed batch 74/357\n",
            "Processed batch 75/357\n",
            "Processed batch 76/357\n",
            "Processed batch 77/357\n",
            "Processed batch 78/357\n",
            "Processed batch 79/357\n",
            "Processed batch 80/357\n",
            "Processed batch 81/357\n",
            "Processed batch 82/357\n",
            "Processed batch 83/357\n",
            "Processed batch 84/357\n",
            "Processed batch 85/357\n",
            "Processed batch 86/357\n",
            "Processed batch 87/357\n",
            "Processed batch 88/357\n",
            "Processed batch 89/357\n",
            "Processed batch 90/357\n",
            "Processed batch 91/357\n",
            "Processed batch 92/357\n",
            "Processed batch 93/357\n",
            "Processed batch 94/357\n",
            "Processed batch 95/357\n",
            "Processed batch 96/357\n",
            "Processed batch 97/357\n",
            "Processed batch 98/357\n",
            "Processed batch 99/357\n",
            "Processed batch 100/357\n",
            "Processed batch 101/357\n",
            "Processed batch 102/357\n",
            "Processed batch 103/357\n",
            "Processed batch 104/357\n",
            "Processed batch 105/357\n",
            "Processed batch 106/357\n",
            "Processed batch 107/357\n",
            "Processed batch 108/357\n",
            "Processed batch 109/357\n",
            "Processed batch 110/357\n",
            "Processed batch 111/357\n",
            "Processed batch 112/357\n",
            "Processed batch 113/357\n",
            "Processed batch 114/357\n",
            "Processed batch 115/357\n",
            "Processed batch 116/357\n",
            "Processed batch 117/357\n",
            "Processed batch 118/357\n",
            "Processed batch 119/357\n",
            "Processed batch 120/357\n",
            "Processed batch 121/357\n",
            "Processed batch 122/357\n",
            "Processed batch 123/357\n",
            "Processed batch 124/357\n",
            "Processed batch 125/357\n",
            "Processed batch 126/357\n",
            "Processed batch 127/357\n",
            "Processed batch 128/357\n",
            "Processed batch 129/357\n",
            "Processed batch 130/357\n",
            "Processed batch 131/357\n",
            "Processed batch 132/357\n",
            "Processed batch 133/357\n",
            "Processed batch 134/357\n",
            "Processed batch 135/357\n",
            "Processed batch 136/357\n",
            "Processed batch 137/357\n",
            "Processed batch 138/357\n",
            "Processed batch 139/357\n",
            "Processed batch 140/357\n",
            "Processed batch 141/357\n",
            "Processed batch 142/357\n",
            "Processed batch 143/357\n",
            "Processed batch 144/357\n",
            "Processed batch 145/357\n",
            "Processed batch 146/357\n",
            "Processed batch 147/357\n",
            "Processed batch 148/357\n",
            "Processed batch 149/357\n",
            "Processed batch 150/357\n",
            "Processed batch 151/357\n",
            "Processed batch 152/357\n",
            "Processed batch 153/357\n",
            "Processed batch 154/357\n",
            "Processed batch 155/357\n",
            "Processed batch 156/357\n",
            "Processed batch 157/357\n",
            "Processed batch 158/357\n",
            "Processed batch 159/357\n",
            "Processed batch 160/357\n",
            "Processed batch 161/357\n",
            "Processed batch 162/357\n",
            "Processed batch 163/357\n",
            "Processed batch 164/357\n",
            "Processed batch 165/357\n",
            "Processed batch 166/357\n",
            "Processed batch 167/357\n",
            "Processed batch 168/357\n",
            "Processed batch 169/357\n",
            "Processed batch 170/357\n",
            "Processed batch 171/357\n",
            "Processed batch 172/357\n",
            "Processed batch 173/357\n",
            "Processed batch 174/357\n",
            "Processed batch 175/357\n",
            "Processed batch 176/357\n",
            "Processed batch 177/357\n",
            "Processed batch 178/357\n",
            "Processed batch 179/357\n",
            "Processed batch 180/357\n",
            "Processed batch 181/357\n",
            "Processed batch 182/357\n",
            "Processed batch 183/357\n",
            "Processed batch 184/357\n",
            "Processed batch 185/357\n",
            "Processed batch 186/357\n",
            "Processed batch 187/357\n",
            "Processed batch 188/357\n",
            "Processed batch 189/357\n",
            "Processed batch 190/357\n",
            "Processed batch 191/357\n",
            "Processed batch 192/357\n",
            "Processed batch 193/357\n",
            "Processed batch 194/357\n",
            "Processed batch 195/357\n",
            "Processed batch 196/357\n",
            "Processed batch 197/357\n",
            "Processed batch 198/357\n",
            "Processed batch 199/357\n",
            "Processed batch 200/357\n",
            "Processed batch 201/357\n",
            "Processed batch 202/357\n",
            "Processed batch 203/357\n",
            "Processed batch 204/357\n",
            "Processed batch 205/357\n",
            "Processed batch 206/357\n",
            "Processed batch 207/357\n",
            "Processed batch 208/357\n",
            "Processed batch 209/357\n",
            "Processed batch 210/357\n",
            "Processed batch 211/357\n",
            "Processed batch 212/357\n",
            "Processed batch 213/357\n",
            "Processed batch 214/357\n",
            "Processed batch 215/357\n",
            "Processed batch 216/357\n",
            "Processed batch 217/357\n",
            "Processed batch 218/357\n",
            "Processed batch 219/357\n",
            "Processed batch 220/357\n",
            "Processed batch 221/357\n",
            "Processed batch 222/357\n",
            "Processed batch 223/357\n",
            "Processed batch 224/357\n",
            "Processed batch 225/357\n",
            "Processed batch 226/357\n",
            "Processed batch 227/357\n",
            "Processed batch 228/357\n",
            "Processed batch 229/357\n",
            "Processed batch 230/357\n",
            "Processed batch 231/357\n",
            "Processed batch 232/357\n",
            "Processed batch 233/357\n",
            "Processed batch 234/357\n",
            "Processed batch 235/357\n",
            "Processed batch 236/357\n",
            "Processed batch 237/357\n",
            "Processed batch 238/357\n",
            "Processed batch 239/357\n",
            "Processed batch 240/357\n",
            "Processed batch 241/357\n",
            "Processed batch 242/357\n",
            "Processed batch 243/357\n",
            "Processed batch 244/357\n",
            "Processed batch 245/357\n",
            "Processed batch 246/357\n",
            "Processed batch 247/357\n",
            "Processed batch 248/357\n",
            "Processed batch 249/357\n",
            "Processed batch 250/357\n",
            "Processed batch 251/357\n",
            "Processed batch 252/357\n",
            "Processed batch 253/357\n",
            "Processed batch 254/357\n",
            "Processed batch 255/357\n",
            "Processed batch 256/357\n",
            "Processed batch 257/357\n",
            "Processed batch 258/357\n",
            "Processed batch 259/357\n",
            "Processed batch 260/357\n",
            "Processed batch 261/357\n",
            "Processed batch 262/357\n",
            "Processed batch 263/357\n",
            "Processed batch 264/357\n",
            "Processed batch 265/357\n",
            "Processed batch 266/357\n",
            "Processed batch 267/357\n",
            "Processed batch 268/357\n",
            "Processed batch 269/357\n",
            "Processed batch 270/357\n",
            "Processed batch 271/357\n",
            "Processed batch 272/357\n",
            "Processed batch 273/357\n",
            "Processed batch 274/357\n",
            "Processed batch 275/357\n",
            "Processed batch 276/357\n",
            "Processed batch 277/357\n",
            "Processed batch 278/357\n",
            "Processed batch 279/357\n",
            "Processed batch 280/357\n",
            "Processed batch 281/357\n",
            "Processed batch 282/357\n",
            "Processed batch 283/357\n",
            "Processed batch 284/357\n",
            "Processed batch 285/357\n",
            "Processed batch 286/357\n",
            "Processed batch 287/357\n",
            "Processed batch 288/357\n",
            "Processed batch 289/357\n",
            "Processed batch 290/357\n",
            "Processed batch 291/357\n",
            "Processed batch 292/357\n",
            "Processed batch 293/357\n",
            "Processed batch 294/357\n",
            "Processed batch 295/357\n",
            "Processed batch 296/357\n",
            "Processed batch 297/357\n",
            "Processed batch 298/357\n",
            "Processed batch 299/357\n",
            "Processed batch 300/357\n",
            "Processed batch 301/357\n",
            "Processed batch 302/357\n",
            "Processed batch 303/357\n",
            "Processed batch 304/357\n",
            "Processed batch 305/357\n",
            "Processed batch 306/357\n",
            "Processed batch 307/357\n",
            "Processed batch 308/357\n",
            "Processed batch 309/357\n",
            "Processed batch 310/357\n",
            "Processed batch 311/357\n",
            "Processed batch 312/357\n",
            "Processed batch 313/357\n",
            "Processed batch 314/357\n",
            "Processed batch 315/357\n",
            "Processed batch 316/357\n",
            "Processed batch 317/357\n",
            "Processed batch 318/357\n",
            "Processed batch 319/357\n",
            "Processed batch 320/357\n",
            "Processed batch 321/357\n",
            "Processed batch 322/357\n",
            "Processed batch 323/357\n",
            "Processed batch 324/357\n",
            "Processed batch 325/357\n",
            "Processed batch 326/357\n",
            "Processed batch 327/357\n",
            "Processed batch 328/357\n",
            "Processed batch 329/357\n",
            "Processed batch 330/357\n",
            "Processed batch 331/357\n",
            "Processed batch 332/357\n",
            "Processed batch 333/357\n",
            "Processed batch 334/357\n",
            "Processed batch 335/357\n",
            "Processed batch 336/357\n",
            "Processed batch 337/357\n",
            "Processed batch 338/357\n",
            "Processed batch 339/357\n",
            "Processed batch 340/357\n",
            "Processed batch 341/357\n",
            "Processed batch 342/357\n",
            "Processed batch 343/357\n",
            "Processed batch 344/357\n",
            "Processed batch 345/357\n",
            "Processed batch 346/357\n",
            "Processed batch 347/357\n",
            "Processed batch 348/357\n",
            "Processed batch 349/357\n",
            "Processed batch 350/357\n",
            "Processed batch 351/357\n",
            "Processed batch 352/357\n",
            "Processed batch 353/357\n",
            "Processed batch 354/357\n",
            "Processed batch 355/357\n",
            "Processed batch 356/357\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-07-30 05:01:07,653] A new study created in memory with name: no-name-40a4131e-7da3-41ff-964c-241e0cfe8e07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed batch 357/357\n",
            "⚖️ Scaling features...\n",
            "🔍 Applying PCA...\n",
            "PCA explained variance: 0.942\n",
            "🤖 Training CatBoost classifier...\n",
            "Starting Optuna hyperparameter optimization...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "220b3ae94b9d4219bdffd14a6b223f8b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[I 2025-07-30 05:02:17,779] Trial 0 finished with value: 0.8731408573928259 and parameters: {'iterations': 437, 'depth': 10, 'learning_rate': 0.22227824312530747, 'l2_leaf_reg': 6.387926357773329, 'border_count': 66}. Best is trial 0 with value: 0.8731408573928259.\n",
            "[I 2025-07-30 05:02:20,524] Trial 1 finished with value: 0.8608923884514436 and parameters: {'iterations': 240, 'depth': 4, 'learning_rate': 0.2611910822747312, 'l2_leaf_reg': 6.41003510568888, 'border_count': 190}. Best is trial 0 with value: 0.8731408573928259.\n",
            "[I 2025-07-30 05:02:41,915] Trial 2 finished with value: 0.8705161854768154 and parameters: {'iterations': 118, 'depth': 10, 'learning_rate': 0.2514083658321223, 'l2_leaf_reg': 2.9110519961044856, 'border_count': 72}. Best is trial 0 with value: 0.8731408573928259.\n",
            "[I 2025-07-30 05:02:46,838] Trial 3 finished with value: 0.8696412948381452 and parameters: {'iterations': 265, 'depth': 6, 'learning_rate': 0.16217936517334897, 'l2_leaf_reg': 4.887505167779041, 'border_count': 97}. Best is trial 0 with value: 0.8731408573928259.\n",
            "[I 2025-07-30 05:02:55,291] Trial 4 finished with value: 0.8608923884514436 and parameters: {'iterations': 651, 'depth': 4, 'learning_rate': 0.09472194807521325, 'l2_leaf_reg': 4.297256589643226, 'border_count': 134}. Best is trial 0 with value: 0.8731408573928259.\n",
            "[I 2025-07-30 05:03:02,031] Trial 5 finished with value: 0.8862642169728784 and parameters: {'iterations': 807, 'depth': 5, 'learning_rate': 0.15912798713994736, 'l2_leaf_reg': 6.331731119758382, 'border_count': 42}. Best is trial 5 with value: 0.8862642169728784.\n",
            "[I 2025-07-30 05:03:19,436] Trial 6 finished with value: 0.8460192475940508 and parameters: {'iterations': 647, 'depth': 5, 'learning_rate': 0.02886496196573106, 'l2_leaf_reg': 9.539969835279999, 'border_count': 248}. Best is trial 5 with value: 0.8862642169728784.\n",
            "[I 2025-07-30 05:03:43,616] Trial 7 finished with value: 0.8696412948381452 and parameters: {'iterations': 828, 'depth': 6, 'learning_rate': 0.03832491306185132, 'l2_leaf_reg': 7.158097238609412, 'border_count': 130}. Best is trial 5 with value: 0.8862642169728784.\n",
            "[I 2025-07-30 05:03:52,373] Trial 8 finished with value: 0.7970253718285214 and parameters: {'iterations': 209, 'depth': 7, 'learning_rate': 0.019972671123413333, 'l2_leaf_reg': 9.18388361870904, 'border_count': 89}. Best is trial 5 with value: 0.8862642169728784.\n",
            "[I 2025-07-30 05:04:05,928] Trial 9 finished with value: 0.884514435695538 and parameters: {'iterations': 696, 'depth': 6, 'learning_rate': 0.16081972614156514, 'l2_leaf_reg': 5.920392514089517, 'border_count': 73}. Best is trial 5 with value: 0.8862642169728784.\n",
            "[I 2025-07-30 05:04:38,924] Trial 10 finished with value: 0.8915135608048994 and parameters: {'iterations': 978, 'depth': 8, 'learning_rate': 0.10264378756176612, 'l2_leaf_reg': 1.161656880533375, 'border_count': 35}. Best is trial 10 with value: 0.8915135608048994.\n",
            "[I 2025-07-30 05:05:11,859] Trial 11 finished with value: 0.8915135608048994 and parameters: {'iterations': 942, 'depth': 8, 'learning_rate': 0.10469666243062777, 'l2_leaf_reg': 1.7702656156718994, 'border_count': 32}. Best is trial 10 with value: 0.8915135608048994.\n",
            "[I 2025-07-30 05:05:46,902] Trial 12 finished with value: 0.8923884514435696 and parameters: {'iterations': 994, 'depth': 8, 'learning_rate': 0.09289774944907872, 'l2_leaf_reg': 1.3036705076856752, 'border_count': 35}. Best is trial 12 with value: 0.8923884514435696.\n",
            "[I 2025-07-30 05:07:30,866] Trial 13 finished with value: 0.8915135608048994 and parameters: {'iterations': 973, 'depth': 8, 'learning_rate': 0.09481066495853699, 'l2_leaf_reg': 1.0388397775802005, 'border_count': 176}. Best is trial 12 with value: 0.8923884514435696.\n",
            "[I 2025-07-30 05:09:23,899] Trial 14 finished with value: 0.8888888888888888 and parameters: {'iterations': 853, 'depth': 9, 'learning_rate': 0.06615035011967307, 'l2_leaf_reg': 2.8270650876845727, 'border_count': 109}. Best is trial 12 with value: 0.8923884514435696.\n",
            "[I 2025-07-30 05:09:39,496] Trial 15 finished with value: 0.8923884514435696 and parameters: {'iterations': 471, 'depth': 8, 'learning_rate': 0.19934962751188368, 'l2_leaf_reg': 3.155497947190561, 'border_count': 32}. Best is trial 12 with value: 0.8923884514435696.\n",
            "[I 2025-07-30 05:11:24,261] Trial 16 finished with value: 0.8792650918635171 and parameters: {'iterations': 467, 'depth': 9, 'learning_rate': 0.19464181909299802, 'l2_leaf_reg': 3.301676443469198, 'border_count': 181}. Best is trial 12 with value: 0.8923884514435696.\n",
            "[I 2025-07-30 05:11:35,530] Trial 17 finished with value: 0.8827646544181977 and parameters: {'iterations': 401, 'depth': 7, 'learning_rate': 0.28994800435898194, 'l2_leaf_reg': 2.249315537973288, 'border_count': 60}. Best is trial 12 with value: 0.8923884514435696.\n",
            "[I 2025-07-30 05:13:56,199] Trial 18 finished with value: 0.8836395450568679 and parameters: {'iterations': 535, 'depth': 9, 'learning_rate': 0.13040991874535698, 'l2_leaf_reg': 3.8998166429324184, 'border_count': 215}. Best is trial 12 with value: 0.8923884514435696.\n",
            "[I 2025-07-30 05:14:20,986] Trial 19 finished with value: 0.8836395450568679 and parameters: {'iterations': 349, 'depth': 8, 'learning_rate': 0.19500724212879386, 'l2_leaf_reg': 2.1504189185437603, 'border_count': 110}. Best is trial 12 with value: 0.8923884514435696.\n",
            "[I 2025-07-30 05:14:55,665] Trial 20 finished with value: 0.8888888888888888 and parameters: {'iterations': 567, 'depth': 7, 'learning_rate': 0.1309798007462633, 'l2_leaf_reg': 4.770214167977721, 'border_count': 160}. Best is trial 12 with value: 0.8923884514435696.\n",
            "[I 2025-07-30 05:15:31,966] Trial 21 finished with value: 0.8862642169728784 and parameters: {'iterations': 906, 'depth': 8, 'learning_rate': 0.05979372997733018, 'l2_leaf_reg': 1.0240060689893642, 'border_count': 48}. Best is trial 12 with value: 0.8923884514435696.\n",
            "[I 2025-07-30 05:16:25,017] Trial 22 finished with value: 0.8871391076115486 and parameters: {'iterations': 997, 'depth': 9, 'learning_rate': 0.12488801580952734, 'l2_leaf_reg': 1.6975631533451412, 'border_count': 32}. Best is trial 12 with value: 0.8923884514435696.\n",
            "[I 2025-07-30 05:16:57,700] Trial 23 finished with value: 0.8932633420822397 and parameters: {'iterations': 733, 'depth': 8, 'learning_rate': 0.1916788849299687, 'l2_leaf_reg': 3.1706022914855856, 'border_count': 55}. Best is trial 23 with value: 0.8932633420822397.\n",
            "[I 2025-07-30 05:17:17,303] Trial 24 finished with value: 0.8976377952755905 and parameters: {'iterations': 777, 'depth': 7, 'learning_rate': 0.1997415267858879, 'l2_leaf_reg': 3.5707277286032504, 'border_count': 56}. Best is trial 24 with value: 0.8976377952755905.\n",
            "[I 2025-07-30 05:17:43,082] Trial 25 finished with value: 0.8906386701662292 and parameters: {'iterations': 747, 'depth': 7, 'learning_rate': 0.2263801098438176, 'l2_leaf_reg': 3.9991591892754252, 'border_count': 83}. Best is trial 24 with value: 0.8976377952755905.\n",
            "[I 2025-07-30 05:18:01,557] Trial 26 finished with value: 0.889763779527559 and parameters: {'iterations': 762, 'depth': 7, 'learning_rate': 0.23603417867038679, 'l2_leaf_reg': 2.5011056079066463, 'border_count': 58}. Best is trial 24 with value: 0.8976377952755905.\n",
            "[I 2025-07-30 05:18:24,859] Trial 27 finished with value: 0.8915135608048994 and parameters: {'iterations': 877, 'depth': 6, 'learning_rate': 0.19346281390107223, 'l2_leaf_reg': 3.416310076280167, 'border_count': 114}. Best is trial 24 with value: 0.8976377952755905.\n",
            "[I 2025-07-30 05:19:07,678] Trial 28 finished with value: 0.884514435695538 and parameters: {'iterations': 586, 'depth': 9, 'learning_rate': 0.1756006668390691, 'l2_leaf_reg': 5.194130800247339, 'border_count': 54}. Best is trial 24 with value: 0.8976377952755905.\n",
            "[I 2025-07-30 05:21:21,602] Trial 29 finished with value: 0.8810148731408574 and parameters: {'iterations': 750, 'depth': 10, 'learning_rate': 0.21006171433119128, 'l2_leaf_reg': 7.450471687608263, 'border_count': 72}. Best is trial 24 with value: 0.8976377952755905.\n",
            "Best parameters: {'iterations': 777, 'depth': 7, 'learning_rate': 0.1997415267858879, 'l2_leaf_reg': 3.5707277286032504, 'border_count': 56}\n",
            "Best validation accuracy: 0.8976\n",
            "🧠 Initializing SHAP explainer...\n",
            "✅ Model training completed!\n",
            "🔄 Preprocessing image...\n",
            "🎯 Extracting features...\n",
            "Extracting ViT features in 1 batches...\n",
            "Processed batch 1/1\n",
            "🤖 Making prediction...\n",
            "🧠 Generating SHAP explanation...\n",
            "\n",
            "============================================================\n",
            "🧠 BRAIN TUMOR DETECTION RESULTS\n",
            "============================================================\n",
            "🔍 **Prediction:** glioma\n",
            "📈 **Confidence Score:** 96.25%\n",
            "🧠 **Top Influencing Features:** PC3, PC4, PC6\n",
            "🩺 **Medical Insight:** The model identified characteristics typical of glioma tumors. Key feature PC3 showed high impact, which often correlates with infiltrative growth patterns commonly seen in gliomas.\n",
            "\n",
            "📊 **Class Probabilities:**\n",
            "   glioma: 96.25%\n",
            "   meningioma: 2.73%\n",
            "   no_tumor: 0.14%\n",
            "   pituitary: 0.89%\n",
            "\n",
            "⚙️ **Technical Details:**\n",
            "   • Model Accuracy: 89.76%\n",
            "   • Prediction Time: 7.29s\n",
            "   • PCA Explained Variance: 0.942\n",
            "============================================================\n",
            "✅ Markdown report saved to: ./reports/brain_tumor_report_20250730_052150.md\n",
            "✅ PDF report saved to: ./reports/brain_tumor_report_20250730_052150.pdf\n",
            "\n",
            "📄 Reports generated:\n",
            "   • Markdown: ./reports/brain_tumor_report_20250730_052150.md\n",
            "   • PDF: ./reports/brain_tumor_report_20250730_052150.pdf\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix,\n",
        "                            precision_score, recall_score, f1_score)\n",
        "import cv2\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Deep Learning imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import timm\n",
        "\n",
        "# CatBoost and Optuna imports\n",
        "from catboost import CatBoostClassifier\n",
        "import optuna\n",
        "from optuna.samplers import TPESampler\n",
        "\n",
        "# Explainable AI imports\n",
        "import shap\n",
        "\n",
        "# Report generation imports\n",
        "from reportlab.lib.pagesizes import letter\n",
        "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image as ReportImage, Table, TableStyle\n",
        "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
        "from reportlab.lib.units import inch\n",
        "from reportlab.lib.colors import Color\n",
        "import matplotlib.pyplot as plt\n",
        "from io import BytesIO\n",
        "import base64\n",
        "from datetime import datetime\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "class MRIPreprocessor:\n",
        "    \"\"\"Enhanced MRI image preprocessing with CLAHE\"\"\"\n",
        "\n",
        "    def __init__(self, target_size=(224, 224)):\n",
        "        self.target_size = target_size\n",
        "        self.clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
        "\n",
        "    def apply_clahe(self, image):\n",
        "        \"\"\"Apply Contrast Limited Adaptive Histogram Equalization\"\"\"\n",
        "        if len(image.shape) == 3:\n",
        "            lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)\n",
        "            lab[:, :, 0] = self.clahe.apply(lab[:, :, 0])\n",
        "            image = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)\n",
        "        else:\n",
        "            image = self.clahe.apply(image)\n",
        "        return image\n",
        "\n",
        "    def preprocess_image(self, image_path):\n",
        "        \"\"\"Complete preprocessing pipeline\"\"\"\n",
        "        if isinstance(image_path, str):\n",
        "            image = cv2.imread(str(image_path))\n",
        "        else:\n",
        "            # Handle PIL Image or numpy array\n",
        "            if hasattr(image_path, 'convert'):\n",
        "                image = np.array(image_path.convert('RGB'))\n",
        "            else:\n",
        "                image = image_path\n",
        "\n",
        "        if image is None:\n",
        "            raise ValueError(f\"Could not load image: {image_path}\")\n",
        "\n",
        "        if len(image.shape) == 3 and image.shape[2] == 3:\n",
        "            if np.max(image) <= 1.0:\n",
        "                image = (image * 255).astype(np.uint8)\n",
        "\n",
        "        # Apply CLAHE\n",
        "        image = self.apply_clahe(image)\n",
        "\n",
        "        # Resize to target size\n",
        "        image = cv2.resize(image, self.target_size)\n",
        "        image = image.astype(np.float32) / 255.0\n",
        "        return image\n",
        "\n",
        "    def batch_preprocess(self, image_paths, labels=None):\n",
        "        \"\"\"Preprocess multiple images with progress tracking\"\"\"\n",
        "        processed_images = []\n",
        "        valid_labels = []\n",
        "        total = len(image_paths)\n",
        "\n",
        "        print(f\"Processing {total} images...\")\n",
        "\n",
        "        for i, path in enumerate(image_paths):\n",
        "            try:\n",
        "                img = self.preprocess_image(path)\n",
        "                processed_images.append(img)\n",
        "                if labels is not None:\n",
        "                    valid_labels.append(labels[i])\n",
        "\n",
        "                # Progress indicator\n",
        "                if (i + 1) % 100 == 0 or (i + 1) == total:\n",
        "                    print(f\"Processed {i + 1}/{total} images ({(i + 1)/total*100:.1f}%)\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {path}: {e}\")\n",
        "                continue\n",
        "\n",
        "        return np.array(processed_images), (np.array(valid_labels) if labels is not None else None)\n",
        "\n",
        "class ViTFeatureExtractor:\n",
        "    \"\"\"Vision Transformer (ViT-B16) feature extraction\"\"\"\n",
        "\n",
        "    def __init__(self, device='cuda', output_dim=64):\n",
        "        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
        "        print(f\"Using device: {self.device}\")\n",
        "\n",
        "        # Load pre-trained ViT-B16\n",
        "        self.model = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=0)\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "\n",
        "        # Add a projection layer to get desired output dimension\n",
        "        self.projection = nn.Linear(self.model.num_features, output_dim).to(self.device)\n",
        "        self.projection.eval()\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    def extract_features(self, images, batch_size=16):\n",
        "        \"\"\"Extract 64-dimensional features using ViT-B16\"\"\"\n",
        "        features = []\n",
        "        total_batches = (len(images) + batch_size - 1) // batch_size\n",
        "\n",
        "        print(f\"Extracting ViT features in {total_batches} batches...\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_idx in range(0, len(images), batch_size):\n",
        "                batch_images = images[batch_idx:batch_idx + batch_size]\n",
        "                batch_tensors = []\n",
        "\n",
        "                for image in batch_images:\n",
        "                    if image.max() <= 1.0:\n",
        "                        image = (image * 255).astype(np.uint8)\n",
        "                    input_tensor = self.transform(image)\n",
        "                    batch_tensors.append(input_tensor)\n",
        "\n",
        "                batch_tensor = torch.stack(batch_tensors).to(self.device)\n",
        "\n",
        "                # Extract features from ViT\n",
        "                vit_features = self.model(batch_tensor)\n",
        "\n",
        "                # Project to desired dimension\n",
        "                projected_features = self.projection(vit_features)\n",
        "\n",
        "                features.append(projected_features.cpu().numpy())\n",
        "\n",
        "                # Progress indicator\n",
        "                current_batch = batch_idx // batch_size + 1\n",
        "                print(f\"Processed batch {current_batch}/{total_batches}\")\n",
        "\n",
        "        return np.vstack(features)\n",
        "\n",
        "class OptunaCatBoostClassifier:\n",
        "    \"\"\"CatBoost classifier with Optuna hyperparameter optimization\"\"\"\n",
        "\n",
        "    def __init__(self, n_trials=50):\n",
        "        self.n_trials = n_trials\n",
        "        self.best_model = None\n",
        "        self.best_params = None\n",
        "        self.study = None\n",
        "\n",
        "    def objective(self, trial, X_train, y_train, X_val, y_val):\n",
        "        \"\"\"Optuna objective function for hyperparameter tuning\"\"\"\n",
        "        params = {\n",
        "            'iterations': trial.suggest_int('iterations', 100, 1000),\n",
        "            'depth': trial.suggest_int('depth', 4, 10),\n",
        "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
        "            'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10),\n",
        "            'border_count': trial.suggest_int('border_count', 32, 255),\n",
        "            'random_seed': 42,\n",
        "            'verbose': False\n",
        "        }\n",
        "\n",
        "        model = CatBoostClassifier(**params)\n",
        "        model.fit(X_train, y_train, eval_set=(X_val, y_val), verbose=False)\n",
        "\n",
        "        y_pred = model.predict(X_val)\n",
        "        accuracy = accuracy_score(y_val, y_pred)\n",
        "\n",
        "        return accuracy\n",
        "\n",
        "    def optimize_and_train(self, X, y):\n",
        "        \"\"\"Optimize hyperparameters and train the best model\"\"\"\n",
        "        print(\"Starting Optuna hyperparameter optimization...\")\n",
        "\n",
        "        # Split for validation\n",
        "        X_train, X_val, y_train, y_val = train_test_split(\n",
        "            X, y, test_size=0.2, random_state=42, stratify=y\n",
        "        )\n",
        "\n",
        "        # Create study\n",
        "        self.study = optuna.create_study(\n",
        "            direction='maximize',\n",
        "            sampler=TPESampler(seed=42)\n",
        "        )\n",
        "\n",
        "        # Optimize\n",
        "        self.study.optimize(\n",
        "            lambda trial: self.objective(trial, X_train, y_train, X_val, y_val),\n",
        "            n_trials=self.n_trials,\n",
        "            show_progress_bar=True\n",
        "        )\n",
        "\n",
        "        # Get best parameters\n",
        "        self.best_params = self.study.best_params\n",
        "        print(f\"Best parameters: {self.best_params}\")\n",
        "        print(f\"Best validation accuracy: {self.study.best_value:.4f}\")\n",
        "\n",
        "        # Train final model with best parameters\n",
        "        self.best_model = CatBoostClassifier(**self.best_params, verbose=False)\n",
        "\n",
        "        # Use full training data for final model\n",
        "        self.best_model.fit(X, y, verbose=False)\n",
        "\n",
        "        return self.best_model\n",
        "\n",
        "    def predict(self, X):\n",
        "        if self.best_model is None:\n",
        "            raise ValueError(\"Model not trained yet!\")\n",
        "        return self.best_model.predict(X)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        if self.best_model is None:\n",
        "            raise ValueError(\"Model not trained yet!\")\n",
        "        return self.best_model.predict_proba(X)\n",
        "\n",
        "    def get_feature_importance(self):\n",
        "        if self.best_model is None:\n",
        "            raise ValueError(\"Model not trained yet!\")\n",
        "        return self.best_model.get_feature_importance()\n",
        "\n",
        "class SHAPExplainer:\n",
        "    \"\"\"SHAP-based explainable AI for model interpretation\"\"\"\n",
        "\n",
        "    def __init__(self, model, X_train_sample):\n",
        "        self.model = model\n",
        "        # Use a smaller sample for SHAP background\n",
        "        self.background = X_train_sample[:min(100, len(X_train_sample))]\n",
        "        self.explainer = shap.Explainer(self.model.predict, self.background)\n",
        "\n",
        "    def explain_prediction(self, X_instance, feature_names=None):\n",
        "        \"\"\"Generate SHAP explanation for a single prediction\"\"\"\n",
        "        if len(X_instance.shape) == 1:\n",
        "            X_instance = X_instance.reshape(1, -1)\n",
        "\n",
        "        shap_values = self.explainer(X_instance)\n",
        "\n",
        "        if feature_names is None:\n",
        "            feature_names = [f'PC{i+1}' for i in range(X_instance.shape[1])]\n",
        "\n",
        "        # Get SHAP values for the prediction\n",
        "        if len(shap_values.shape) == 3:  # Multi-class\n",
        "            shap_vals = shap_values.values[0]  # First instance\n",
        "        else:\n",
        "            shap_vals = shap_values.values[0]\n",
        "\n",
        "        # Get top influential features\n",
        "        if len(shap_vals.shape) == 2:  # Multi-class\n",
        "            # Sum absolute SHAP values across classes\n",
        "            importance = np.abs(shap_vals).sum(axis=1)\n",
        "        else:\n",
        "            importance = np.abs(shap_vals)\n",
        "\n",
        "        top_indices = np.argsort(importance)[::-1][:5]\n",
        "        top_features = [(feature_names[i], importance[i]) for i in top_indices]\n",
        "\n",
        "        return {\n",
        "            'shap_values': shap_values,\n",
        "            'top_features': top_features,\n",
        "            'feature_importance': importance\n",
        "        }\n",
        "\n",
        "    def plot_explanation(self, shap_values, save_path=None):\n",
        "        \"\"\"Create SHAP explanation plots\"\"\"\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "        # Waterfall plot\n",
        "        if len(shap_values.shape) == 3:\n",
        "            shap.plots.waterfall(shap_values[0], show=False, max_display=10)\n",
        "        else:\n",
        "            shap.plots.waterfall(shap_values[0], show=False, max_display=10)\n",
        "\n",
        "        plt.sca(ax1)\n",
        "        plt.title(\"SHAP Waterfall Plot\")\n",
        "\n",
        "        # Summary plot\n",
        "        plt.sca(ax2)\n",
        "        if len(shap_values.values.shape) == 3:\n",
        "            shap.plots.bar(shap_values[0], show=False, max_display=10)\n",
        "        else:\n",
        "            shap.plots.bar(shap_values[0], show=False, max_display=10)\n",
        "        plt.title(\"Feature Importance\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        if save_path:\n",
        "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "\n",
        "        return fig\n",
        "\n",
        "class ReportGenerator:\n",
        "    \"\"\"Generate comprehensive PDF and Markdown reports\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.styles = getSampleStyleSheet()\n",
        "        self.custom_style = ParagraphStyle(\n",
        "            'CustomTitle',\n",
        "            parent=self.styles['Heading1'],\n",
        "            fontSize=16,\n",
        "            textColor=Color(0.2, 0.4, 0.6),\n",
        "            spaceAfter=12\n",
        "        )\n",
        "\n",
        "    def generate_markdown_report(self, prediction_result, save_path=\"brain_tumor_report.md\"):\n",
        "        \"\"\"Generate detailed markdown report\"\"\"\n",
        "\n",
        "        report_content = f\"\"\"# 🧠 Brain Tumor Detection Report\n",
        "\n",
        "**Generated on:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "\n",
        "## 📊 Prediction Results\n",
        "\n",
        "🔍 **Prediction:** {prediction_result['predicted_class']}\n",
        "📈 **Confidence Score:** {prediction_result['confidence']:.2f}%\n",
        "🧠 **Top Influencing Features:** {', '.join([f[0] for f in prediction_result['top_features'][:3]])}\n",
        "\n",
        "## 🩺 Medical Insight\n",
        "\n",
        "{prediction_result['medical_insight']}\n",
        "\n",
        "## 📋 Detailed Analysis\n",
        "\n",
        "### Preprocessing Steps:\n",
        "1. ✅ **CLAHE Applied:** Contrast-Limited Adaptive Histogram Equalization for enhanced image quality\n",
        "2. ✅ **Image Resized:** Standardized to 224x224 pixels for ViT compatibility\n",
        "3. ✅ **Normalization:** Pixel values normalized to [0,1] range\n",
        "\n",
        "### Feature Extraction:\n",
        "- **Model Used:** Vision Transformer (ViT-B16) pre-trained\n",
        "- **Feature Dimension:** 64-dimensional deep features extracted\n",
        "- **PCA Reduction:** Dimensionality reduced to 45 components\n",
        "- **Explained Variance:** {prediction_result.get('explained_variance', 'N/A'):.3f}\n",
        "\n",
        "### Classification:\n",
        "- **Algorithm:** CatBoost Classifier\n",
        "- **Hyperparameter Optimization:** Optuna-tuned parameters\n",
        "- **Best Parameters:** {prediction_result.get('best_params', 'N/A')}\n",
        "- **Model Accuracy:** {prediction_result.get('model_accuracy', 'N/A'):.2f}%\n",
        "\n",
        "## 🎯 Class Probabilities\n",
        "\n",
        "| Class | Probability | Confidence |\n",
        "|-------|-------------|------------|\n",
        "\"\"\"\n",
        "\n",
        "        for class_name, prob in prediction_result['class_probabilities'].items():\n",
        "            report_content += f\"| {class_name} | {prob:.4f} | {prob*100:.2f}% |\\n\"\n",
        "\n",
        "        report_content += f\"\"\"\n",
        "\n",
        "## 🔍 SHAP Feature Analysis\n",
        "\n",
        "### Top 5 Most Influential Features:\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "        for i, (feature, importance) in enumerate(prediction_result['top_features'][:5], 1):\n",
        "            report_content += f\"{i}. **{feature}:** Impact score {importance:.4f}\\n\"\n",
        "\n",
        "        report_content += f\"\"\"\n",
        "\n",
        "### Feature Interpretation:\n",
        "{prediction_result.get('feature_interpretation', 'Feature analysis completed using SHAP values to identify key contributing factors.')}\n",
        "\n",
        "## 📈 Model Performance Summary\n",
        "\n",
        "- **Training Samples:** {prediction_result.get('training_samples', 'N/A')}\n",
        "- **Feature Extraction Time:** {prediction_result.get('extraction_time', 'N/A')}\n",
        "- **Prediction Time:** {prediction_result.get('prediction_time', 'N/A')}\n",
        "- **Cross-validation Score:** {prediction_result.get('cv_score', 'N/A')}\n",
        "\n",
        "## ⚠️ Important Notes\n",
        "\n",
        "1. This prediction is based on automated analysis and should be verified by medical professionals\n",
        "2. The model was trained on publicly available datasets and may not cover all tumor variations\n",
        "3. SHAP explanations provide insights into feature importance but require clinical interpretation\n",
        "4. For critical medical decisions, always consult with qualified radiologists and oncologists\n",
        "\n",
        "---\n",
        "*Report generated by AI-powered Brain Tumor Classification System*\n",
        "\"\"\"\n",
        "\n",
        "        with open(save_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(report_content)\n",
        "\n",
        "        print(f\"✅ Markdown report saved to: {save_path}\")\n",
        "        return save_path\n",
        "\n",
        "    def generate_pdf_report(self, prediction_result, save_path=\"brain_tumor_report.pdf\"):\n",
        "        \"\"\"Generate comprehensive PDF report\"\"\"\n",
        "\n",
        "        doc = SimpleDocTemplate(save_path, pagesize=letter)\n",
        "        story = []\n",
        "\n",
        "        # Title\n",
        "        title = Paragraph(\"🧠 Brain Tumor Detection Report\", self.custom_style)\n",
        "        story.append(title)\n",
        "        story.append(Spacer(1, 12))\n",
        "\n",
        "        # Prediction Results\n",
        "        pred_text = f\"\"\"\n",
        "        <b>🔍 Prediction:</b> {prediction_result['predicted_class']}<br/>\n",
        "        <b>📈 Confidence Score:</b> {prediction_result['confidence']:.2f}%<br/>\n",
        "        <b>🧠 Top Features:</b> {', '.join([f[0] for f in prediction_result['top_features'][:3]])}<br/>\n",
        "        \"\"\"\n",
        "        story.append(Paragraph(pred_text, self.styles['Normal']))\n",
        "        story.append(Spacer(1, 12))\n",
        "\n",
        "        # Medical Insight\n",
        "        insight_text = f\"<b>🩺 Medical Insight:</b><br/>{prediction_result['medical_insight']}\"\n",
        "        story.append(Paragraph(insight_text, self.styles['Normal']))\n",
        "        story.append(Spacer(1, 12))\n",
        "\n",
        "        # Class Probabilities Table\n",
        "        prob_data = [['Class', 'Probability', 'Confidence']]\n",
        "        for class_name, prob in prediction_result['class_probabilities'].items():\n",
        "            prob_data.append([class_name, f\"{prob:.4f}\", f\"{prob*100:.2f}%\"])\n",
        "\n",
        "        prob_table = Table(prob_data)\n",
        "        prob_table.setStyle(TableStyle([\n",
        "            ('BACKGROUND', (0, 0), (-1, 0), Color(0.8, 0.8, 0.8)),\n",
        "            ('TEXTCOLOR', (0, 0), (-1, 0), Color(0, 0, 0)),\n",
        "            ('ALIGN', (0, 0), (-1, -1), 'CENTER'),\n",
        "            ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n",
        "            ('FONTSIZE', (0, 0), (-1, 0), 12),\n",
        "            ('BOTTOMPADDING', (0, 0), (-1, 0), 12),\n",
        "            ('BACKGROUND', (0, 1), (-1, -1), Color(0.95, 0.95, 0.95)),\n",
        "        ]))\n",
        "\n",
        "        story.append(prob_table)\n",
        "        story.append(Spacer(1, 12))\n",
        "\n",
        "        # Technical Details\n",
        "        tech_text = f\"\"\"\n",
        "        <b>Technical Details:</b><br/>\n",
        "        • Preprocessing: CLAHE + Resize to 224x224<br/>\n",
        "        • Feature Extractor: ViT-B16 (64-dim features)<br/>\n",
        "        • Dimensionality Reduction: PCA to 45 components<br/>\n",
        "        • Classifier: CatBoost with Optuna optimization<br/>\n",
        "        • Model Accuracy: {prediction_result.get('model_accuracy', 'N/A'):.2f}%<br/>\n",
        "        \"\"\"\n",
        "        story.append(Paragraph(tech_text, self.styles['Normal']))\n",
        "\n",
        "        doc.build(story)\n",
        "        print(f\"✅ PDF report saved to: {save_path}\")\n",
        "        return save_path\n",
        "\n",
        "class BrainTumorClassifier:\n",
        "    \"\"\"Main class orchestrating the entire pipeline\"\"\"\n",
        "\n",
        "    def __init__(self, pca_components=45):\n",
        "        self.preprocessor = MRIPreprocessor()\n",
        "        self.feature_extractor = ViTFeatureExtractor(output_dim=64)\n",
        "        self.scaler = StandardScaler()\n",
        "        self.pca = PCA(n_components=pca_components)\n",
        "        self.classifier = OptunaCatBoostClassifier(n_trials=30)\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.shap_explainer = None\n",
        "        self.report_generator = ReportGenerator()\n",
        "        self.results = {}\n",
        "        self.class_mapping = {\n",
        "            'notumor': 'no_tumor',\n",
        "            'glioma': 'glioma',\n",
        "            'meningioma': 'meningioma',\n",
        "            'pituitary': 'pituitary'\n",
        "        }\n",
        "        self.model_trained = False\n",
        "\n",
        "    def load_training_data(self, base_path):\n",
        "        \"\"\"Load training dataset\"\"\"\n",
        "        print(\"Loading training data...\")\n",
        "\n",
        "        image_paths = []\n",
        "        labels = []\n",
        "\n",
        "        # Load from directory structure\n",
        "        for class_folder in os.listdir(base_path):\n",
        "            class_path = os.path.join(base_path, class_folder)\n",
        "            if not os.path.isdir(class_path):\n",
        "                continue\n",
        "\n",
        "            if class_folder not in self.class_mapping:\n",
        "                continue\n",
        "\n",
        "            mapped_label = self.class_mapping[class_folder]\n",
        "\n",
        "            for img_file in os.listdir(class_path):\n",
        "                if img_file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n",
        "                    img_path = os.path.join(class_path, img_file)\n",
        "                    image_paths.append(img_path)\n",
        "                    labels.append(mapped_label)\n",
        "\n",
        "        print(f\"Loaded {len(image_paths)} training images\")\n",
        "        return image_paths, labels\n",
        "\n",
        "    def train_model(self, training_data_path):\n",
        "        \"\"\"Train the complete pipeline\"\"\"\n",
        "        print(\"🚀 Starting model training pipeline...\")\n",
        "\n",
        "        # Load training data\n",
        "        image_paths, labels = self.load_training_data(training_data_path)\n",
        "\n",
        "        # Preprocess images\n",
        "        print(\"🔄 Preprocessing images...\")\n",
        "        images, y = self.preprocessor.batch_preprocess(image_paths, labels)\n",
        "\n",
        "        # Encode labels\n",
        "        y_encoded = self.label_encoder.fit_transform(y)\n",
        "\n",
        "        # Extract ViT features\n",
        "        print(\"🎯 Extracting ViT features...\")\n",
        "        vit_features = self.feature_extractor.extract_features(images)\n",
        "\n",
        "        # Scale features\n",
        "        print(\"⚖️ Scaling features...\")\n",
        "        scaled_features = self.scaler.fit_transform(vit_features)\n",
        "\n",
        "        # Apply PCA\n",
        "        print(\"🔍 Applying PCA...\")\n",
        "        pca_features = self.pca.fit_transform(scaled_features)\n",
        "        print(f\"PCA explained variance: {self.pca.explained_variance_ratio_.sum():.3f}\")\n",
        "\n",
        "        # Train CatBoost with Optuna\n",
        "        print(\"🤖 Training CatBoost classifier...\")\n",
        "        self.classifier.optimize_and_train(pca_features, y_encoded)\n",
        "\n",
        "        # Initialize SHAP explainer\n",
        "        print(\"🧠 Initializing SHAP explainer...\")\n",
        "        self.shap_explainer = SHAPExplainer(self.classifier.best_model, pca_features)\n",
        "\n",
        "        # Store training info\n",
        "        self.results = {\n",
        "            'training_samples': len(images),\n",
        "            'explained_variance': self.pca.explained_variance_ratio_.sum(),\n",
        "            'best_params': self.classifier.best_params,\n",
        "            'model_accuracy': self.classifier.study.best_value * 100\n",
        "        }\n",
        "\n",
        "        self.model_trained = True\n",
        "        print(\"✅ Model training completed!\")\n",
        "\n",
        "    def predict_single_image(self, image_input):\n",
        "        \"\"\"Predict a single image with comprehensive analysis\"\"\"\n",
        "        if not self.model_trained:\n",
        "            raise ValueError(\"Model not trained yet! Call train_model() first.\")\n",
        "\n",
        "        start_time = datetime.now()\n",
        "\n",
        "        # Preprocess image\n",
        "        print(\"🔄 Preprocessing image...\")\n",
        "        processed_image = self.preprocessor.preprocess_image(image_input)\n",
        "\n",
        "        # Extract features\n",
        "        print(\"🎯 Extracting features...\")\n",
        "        vit_features = self.feature_extractor.extract_features([processed_image])\n",
        "        scaled_features = self.scaler.transform(vit_features)\n",
        "        pca_features = self.pca.transform(scaled_features)\n",
        "\n",
        "        # Make prediction\n",
        "        print(\"🤖 Making prediction...\")\n",
        "        prediction = self.classifier.predict(pca_features)[0]\n",
        "        probabilities = self.classifier.predict_proba(pca_features)[0]\n",
        "\n",
        "        # Get SHAP explanation\n",
        "        print(\"🧠 Generating SHAP explanation...\")\n",
        "        shap_result = self.shap_explainer.explain_prediction(pca_features)\n",
        "\n",
        "        # Decode prediction\n",
        "        predicted_class = self.label_encoder.inverse_transform([prediction])[0]\n",
        "\n",
        "        # FIX: Convert numpy array to scalar value\n",
        "        if isinstance(probabilities[prediction], np.ndarray):\n",
        "            confidence = float(probabilities[prediction]) * 100\n",
        "        else:\n",
        "            confidence = probabilities[prediction] * 100\n",
        "\n",
        "        # Create class probabilities dictionary\n",
        "        class_probabilities = {}\n",
        "        for i, class_name in enumerate(self.label_encoder.classes_):\n",
        "            # FIX: Ensure probabilities are scalar values\n",
        "            if isinstance(probabilities[i], np.ndarray):\n",
        "                class_probabilities[class_name] = float(probabilities[i])\n",
        "            else:\n",
        "                class_probabilities[class_name] = probabilities[i]\n",
        "\n",
        "        # Generate medical insight\n",
        "        medical_insight = self.generate_medical_insight(predicted_class, shap_result['top_features'])\n",
        "\n",
        "        prediction_time = (datetime.now() - start_time).total_seconds()\n",
        "\n",
        "        result = {\n",
        "            'predicted_class': predicted_class,\n",
        "            'confidence': confidence,\n",
        "            'class_probabilities': class_probabilities,\n",
        "            'shap_values': shap_result['shap_values'],\n",
        "            'top_features': shap_result['top_features'],\n",
        "            'medical_insight': medical_insight,\n",
        "            'prediction_time': f\"{prediction_time:.2f}s\",\n",
        "            **self.results\n",
        "        }\n",
        "\n",
        "        return result\n",
        "\n",
        "    def generate_medical_insight(self, predicted_class, top_features):\n",
        "        \"\"\"Generate medical insights based on prediction and SHAP values\"\"\"\n",
        "\n",
        "        insights = {\n",
        "            'glioma': f\"The model identified characteristics typical of glioma tumors. Key feature {top_features[0][0]} showed high impact, which often correlates with infiltrative growth patterns commonly seen in gliomas.\",\n",
        "            'meningioma': f\"Features suggest meningioma characteristics. {top_features[0][0]} significantly contributed to this classification, typically associated with well-defined tumor boundaries characteristic of meningiomas.\",\n",
        "            'pituitary': f\"Pituitary tumor features detected. The influential feature {top_features[0][0]} aligns with typical pituitary adenoma presentations in the sella turcica region.\",\n",
        "            'no_tumor': f\"No tumor characteristics identified. Feature {top_features[0][0]} strongly supported the healthy tissue classification, indicating normal brain structure patterns.\"\n",
        "        }\n",
        "\n",
        "        return insights.get(predicted_class, \"Classification completed with SHAP feature analysis.\")\n",
        "\n",
        "    def generate_reports(self, prediction_result, output_dir=\"./reports\"):\n",
        "        \"\"\"Generate both PDF and Markdown reports\"\"\"\n",
        "        if not os.path.exists(output_dir):\n",
        "            os.makedirs(output_dir)\n",
        "\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "        # Generate markdown report\n",
        "        md_path = os.path.join(output_dir, f\"brain_tumor_report_{timestamp}.md\")\n",
        "        self.report_generator.generate_markdown_report(prediction_result, md_path)\n",
        "\n",
        "        # Generate PDF report\n",
        "        pdf_path = os.path.join(output_dir, f\"brain_tumor_report_{timestamp}.pdf\")\n",
        "        self.report_generator.generate_pdf_report(prediction_result, pdf_path)\n",
        "\n",
        "        return md_path, pdf_path\n",
        "\n",
        "    def display_prediction_summary(self, result):\n",
        "        \"\"\"Display formatted prediction summary\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"🧠 BRAIN TUMOR DETECTION RESULTS\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"🔍 **Prediction:** {result['predicted_class']}\")\n",
        "        print(f\"📈 **Confidence Score:** {result['confidence']:.2f}%\")\n",
        "\n",
        "        top_features_str = \", \".join([f[0] for f in result['top_features'][:3]])\n",
        "        print(f\"🧠 **Top Influencing Features:** {top_features_str}\")\n",
        "        print(f\"🩺 **Medical Insight:** {result['medical_insight']}\")\n",
        "\n",
        "        print(f\"\\n📊 **Class Probabilities:**\")\n",
        "        for class_name, prob in result['class_probabilities'].items():\n",
        "            print(f\"   {class_name}: {prob*100:.2f}%\")\n",
        "\n",
        "        print(f\"\\n⚙️ **Technical Details:**\")\n",
        "        print(f\"   • Model Accuracy: {result.get('model_accuracy', 'N/A'):.2f}%\")\n",
        "        print(f\"   • Prediction Time: {result['prediction_time']}\")\n",
        "        print(f\"   • PCA Explained Variance: {result.get('explained_variance', 'N/A'):.3f}\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "# Example usage and demo function\n",
        "def demo_brain_tumor_classifier():\n",
        "    \"\"\"Demonstration of the brain tumor classifier\"\"\"\n",
        "\n",
        "    print(\"🧠 Brain Tumor Classifier Demo\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Initialize classifier\n",
        "    classifier = BrainTumorClassifier(pca_components=45)\n",
        "\n",
        "    # Train model (replace with your training data path)\n",
        "    training_data_path = \"/content/binary_class/Training\"\n",
        "\n",
        "    try:\n",
        "        # Check if training data exists\n",
        "        if os.path.exists(training_data_path):\n",
        "            classifier.train_model(training_data_path)\n",
        "        else:\n",
        "            print(\"⚠️ Training data path not found. Using dummy training for demo...\")\n",
        "            # In practice, you would provide the actual training data\n",
        "\n",
        "        # Example prediction (replace with actual image path)\n",
        "        image_path = \"/content/binary_class/Testing/glioma/Te-gl_0010.jpg\"  # Update this path\n",
        "\n",
        "        if os.path.exists(image_path):\n",
        "            # Make prediction\n",
        "            result = classifier.predict_single_image(image_path)\n",
        "\n",
        "            # Display results\n",
        "            classifier.display_prediction_summary(result)\n",
        "\n",
        "            # Generate reports\n",
        "            md_path, pdf_path = classifier.generate_reports(result)\n",
        "\n",
        "            print(f\"\\n📄 Reports generated:\")\n",
        "            print(f\"   • Markdown: {md_path}\")\n",
        "            print(f\"   • PDF: {pdf_path}\")\n",
        "\n",
        "        else:\n",
        "            print(\"⚠️ Test image path not found. Please provide a valid image path for prediction.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error in demo: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "def create_streamlit_app():\n",
        "    \"\"\"Create a Streamlit web application for brain tumor classification\"\"\"\n",
        "\n",
        "    try:\n",
        "        import streamlit as st\n",
        "        from PIL import Image\n",
        "        import io\n",
        "\n",
        "        st.set_page_config(\n",
        "            page_title=\"🧠 Brain Tumor Classifier\",\n",
        "            page_icon=\"🧠\",\n",
        "            layout=\"wide\"\n",
        "        )\n",
        "\n",
        "        st.title(\"🧠 Brain Tumor Classification System\")\n",
        "        st.markdown(\"---\")\n",
        "\n",
        "        # Sidebar for model information\n",
        "        with st.sidebar:\n",
        "            st.header(\"📋 Model Information\")\n",
        "            st.info(\"\"\"\n",
        "            **Features:**\n",
        "            - ViT-B16 Feature Extraction\n",
        "            - CLAHE Preprocessing\n",
        "            - PCA Dimensionality Reduction\n",
        "            - CatBoost Classification\n",
        "            - SHAP Explainability\n",
        "            - Comprehensive Reporting\n",
        "            \"\"\")\n",
        "\n",
        "            st.header(\"📊 Supported Classes\")\n",
        "            st.write(\"• Glioma\")\n",
        "            st.write(\"• Meningioma\")\n",
        "            st.write(\"• Pituitary\")\n",
        "            st.write(\"• No Tumor\")\n",
        "\n",
        "        # Main content\n",
        "        col1, col2 = st.columns([1, 1])\n",
        "\n",
        "        with col1:\n",
        "            st.header(\"📤 Upload MRI Image\")\n",
        "            uploaded_file = st.file_uploader(\n",
        "                \"Choose an MRI image...\",\n",
        "                type=['png', 'jpg', 'jpeg', 'bmp', 'tiff']\n",
        "            )\n",
        "\n",
        "            if uploaded_file is not None:\n",
        "                # Display uploaded image\n",
        "                image = Image.open(uploaded_file)\n",
        "                st.image(image, caption=\"Uploaded MRI Image\", use_column_width=True)\n",
        "\n",
        "                # Training path input\n",
        "                training_path = st.text_input(\n",
        "                    \"Training Data Path:\",\n",
        "                    placeholder=\"/path/to/training/data\"\n",
        "                )\n",
        "\n",
        "                if st.button(\"🚀 Classify Image\", type=\"primary\"):\n",
        "                    if not training_path:\n",
        "                        st.error(\"Please provide the training data path!\")\n",
        "                        return\n",
        "\n",
        "                    with st.spinner(\"🔄 Processing image and making prediction...\"):\n",
        "                        try:\n",
        "                            # Initialize and train classifier\n",
        "                            classifier = BrainTumorClassifier()\n",
        "\n",
        "                            # Convert uploaded file to format expected by classifier\n",
        "                            image_bytes = io.BytesIO()\n",
        "                            image.save(image_bytes, format='PNG')\n",
        "                            image_bytes.seek(0)\n",
        "\n",
        "                            # Train model if not already trained\n",
        "                            if os.path.exists(training_path):\n",
        "                                classifier.train_model(training_path)\n",
        "                            else:\n",
        "                                st.error(\"Training data path not found!\")\n",
        "                                return\n",
        "\n",
        "                            # Make prediction\n",
        "                            result = classifier.predict_single_image(image)\n",
        "\n",
        "                            # Store result in session state\n",
        "                            st.session_state.prediction_result = result\n",
        "                            st.session_state.classifier = classifier\n",
        "\n",
        "                        except Exception as e:\n",
        "                            st.error(f\"Error during classification: {e}\")\n",
        "                            return\n",
        "\n",
        "        with col2:\n",
        "            st.header(\"📊 Prediction Results\")\n",
        "\n",
        "            if 'prediction_result' in st.session_state:\n",
        "                result = st.session_state.prediction_result\n",
        "\n",
        "                # Main prediction\n",
        "                st.success(f\"🔍 **Prediction:** {result['predicted_class']}\")\n",
        "                st.info(f\"📈 **Confidence:** {result['confidence']:.2f}%\")\n",
        "\n",
        "                # Class probabilities\n",
        "                st.subheader(\"📊 Class Probabilities\")\n",
        "                prob_df = pd.DataFrame([\n",
        "                    {\"Class\": k, \"Probability\": f\"{v*100:.2f}%\", \"Score\": v}\n",
        "                    for k, v in result['class_probabilities'].items()\n",
        "                ]).sort_values('Score', ascending=False)\n",
        "\n",
        "                st.dataframe(prob_df[['Class', 'Probability']], use_container_width=True)\n",
        "\n",
        "                # Top features\n",
        "                st.subheader(\"🧠 Top Influencing Features\")\n",
        "                features_text = \"\\n\".join([\n",
        "                    f\"• **{feat[0]}:** {feat[1]:.4f}\"\n",
        "                    for feat in result['top_features'][:5]\n",
        "                ])\n",
        "                st.markdown(features_text)\n",
        "\n",
        "                # Medical insight\n",
        "                st.subheader(\"🩺 Medical Insight\")\n",
        "                st.write(result['medical_insight'])\n",
        "\n",
        "                # Generate reports\n",
        "                if st.button(\"📄 Generate Reports\"):\n",
        "                    classifier = st.session_state.classifier\n",
        "                    with st.spinner(\"Generating reports...\"):\n",
        "                        md_path, pdf_path = classifier.generate_reports(result)\n",
        "\n",
        "                        col_md, col_pdf = st.columns(2)\n",
        "                        with col_md:\n",
        "                            with open(md_path, 'r') as f:\n",
        "                                st.download_button(\n",
        "                                    label=\"📄 Download Markdown Report\",\n",
        "                                    data=f.read(),\n",
        "                                    file_name=\"brain_tumor_report.md\",\n",
        "                                    mime=\"text/markdown\"\n",
        "                                )\n",
        "\n",
        "                        with col_pdf:\n",
        "                            with open(pdf_path, 'rb') as f:\n",
        "                                st.download_button(\n",
        "                                    label=\"📄 Download PDF Report\",\n",
        "                                    data=f.read(),\n",
        "                                    file_name=\"brain_tumor_report.pdf\",\n",
        "                                    mime=\"application/pdf\"\n",
        "                                )\n",
        "            else:\n",
        "                st.info(\"👆 Upload an MRI image and click 'Classify Image' to see results\")\n",
        "\n",
        "        # Technical details section\n",
        "        st.markdown(\"---\")\n",
        "        st.header(\"⚙️ Technical Details\")\n",
        "\n",
        "        tech_col1, tech_col2, tech_col3 = st.columns(3)\n",
        "\n",
        "        with tech_col1:\n",
        "            st.subheader(\"🔄 Preprocessing\")\n",
        "            st.write(\"• CLAHE Enhancement\")\n",
        "            st.write(\"• Resize to 224×224\")\n",
        "            st.write(\"• Normalization\")\n",
        "\n",
        "        with tech_col2:\n",
        "            st.subheader(\"🎯 Feature Extraction\")\n",
        "            st.write(\"• ViT-B16 Pre-trained\")\n",
        "            st.write(\"• 64-dimensional features\")\n",
        "            st.write(\"• PCA to 45 components\")\n",
        "\n",
        "        with tech_col3:\n",
        "            st.subheader(\"🤖 Classification\")\n",
        "            st.write(\"• CatBoost Classifier\")\n",
        "            st.write(\"• Optuna Optimization\")\n",
        "            st.write(\"• SHAP Explanations\")\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"Streamlit not installed. Install with: pip install streamlit\")\n",
        "\n",
        "def create_gradio_app():\n",
        "    \"\"\"Create a Gradio web interface for brain tumor classification\"\"\"\n",
        "\n",
        "    try:\n",
        "        import gradio as gr\n",
        "        from PIL import Image\n",
        "\n",
        "        # Global classifier instance\n",
        "        global_classifier = None\n",
        "\n",
        "        def classify_image(image, training_path):\n",
        "            \"\"\"Gradio classification function\"\"\"\n",
        "            nonlocal global_classifier\n",
        "\n",
        "            if image is None:\n",
        "                return \"Please upload an image\", \"\", \"\", \"\", \"\"\n",
        "\n",
        "            if not training_path or not os.path.exists(training_path):\n",
        "                return \"Invalid training data path\", \"\", \"\", \"\", \"\"\n",
        "\n",
        "            try:\n",
        "                # Initialize classifier if needed\n",
        "                if global_classifier is None:\n",
        "                    global_classifier = BrainTumorClassifier()\n",
        "                    global_classifier.train_model(training_path)\n",
        "\n",
        "                # Make prediction\n",
        "                result = global_classifier.predict_single_image(image)\n",
        "\n",
        "                # Format results\n",
        "                prediction = f\"🔍 **Prediction:** {result['predicted_class']}\"\n",
        "                confidence = f\"📈 **Confidence:** {result['confidence']:.2f}%\"\n",
        "\n",
        "                top_features = \"🧠 **Top Features:**\\n\" + \"\\n\".join([\n",
        "                    f\"• {feat[0]}: {feat[1]:.4f}\"\n",
        "                    for feat in result['top_features'][:5]\n",
        "                ])\n",
        "\n",
        "                probabilities = \"📊 **Class Probabilities:**\\n\" + \"\\n\".join([\n",
        "                    f\"• {k}: {v*100:.2f}%\"\n",
        "                    for k, v in result['class_probabilities'].items()\n",
        "                ])\n",
        "\n",
        "                medical_insight = f\"🩺 **Medical Insight:**\\n{result['medical_insight']}\"\n",
        "\n",
        "                return prediction, confidence, top_features, probabilities, medical_insight\n",
        "\n",
        "            except Exception as e:\n",
        "                return f\"Error: {str(e)}\", \"\", \"\", \"\", \"\"\n",
        "\n",
        "        # Create Gradio interface\n",
        "        with gr.Blocks(title=\"🧠 Brain Tumor Classifier\") as app:\n",
        "            gr.Markdown(\"# 🧠 Brain Tumor Classification System\")\n",
        "            gr.Markdown(\"Upload an MRI image for automated brain tumor classification using ViT + CatBoost + SHAP\")\n",
        "\n",
        "            with gr.Row():\n",
        "                with gr.Column(scale=1):\n",
        "                    image_input = gr.Image(\n",
        "                        type=\"pil\",\n",
        "                        label=\"📤 Upload MRI Image\"\n",
        "                    )\n",
        "\n",
        "                    training_path_input = gr.Textbox(\n",
        "                        label=\"📁 Training Data Path\",\n",
        "                        placeholder=\"/path/to/training/data\",\n",
        "                        value=\"\"\n",
        "                    )\n",
        "\n",
        "                    classify_btn = gr.Button(\n",
        "                        \"🚀 Classify Image\",\n",
        "                        variant=\"primary\"\n",
        "                    )\n",
        "\n",
        "                with gr.Column(scale=1):\n",
        "                    prediction_output = gr.Markdown(label=\"🔍 Prediction\")\n",
        "                    confidence_output = gr.Markdown(label=\"📈 Confidence\")\n",
        "                    features_output = gr.Markdown(label=\"🧠 Top Features\")\n",
        "                    probabilities_output = gr.Markdown(label=\"📊 Probabilities\")\n",
        "                    insight_output = gr.Markdown(label=\"🩺 Medical Insight\")\n",
        "\n",
        "            # Connect the classification function\n",
        "            classify_btn.click(\n",
        "                fn=classify_image,\n",
        "                inputs=[image_input, training_path_input],\n",
        "                outputs=[\n",
        "                    prediction_output,\n",
        "                    confidence_output,\n",
        "                    features_output,\n",
        "                    probabilities_output,\n",
        "                    insight_output\n",
        "                ]\n",
        "            )\n",
        "\n",
        "            # Add examples section\n",
        "            gr.Markdown(\"## 📋 Technical Specifications\")\n",
        "            gr.Markdown(\"\"\"\n",
        "            - **Preprocessing:** CLAHE + Resize to 224×224\n",
        "            - **Feature Extraction:** Vision Transformer (ViT-B16)\n",
        "            - **Dimensionality Reduction:** PCA to 45 components\n",
        "            - **Classification:** CatBoost with Optuna optimization\n",
        "            - **Explainability:** SHAP feature importance analysis\n",
        "            - **Supported Classes:** Glioma, Meningioma, Pituitary, No Tumor\n",
        "            \"\"\")\n",
        "\n",
        "        return app\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"Gradio not installed. Install with: pip install gradio\")\n",
        "        return None\n",
        "\n",
        "# Command-line interface\n",
        "def main():\n",
        "    \"\"\"Main function with command-line argument support\"\"\"\n",
        "    # Check if running in Colab\n",
        "    try:\n",
        "        import google.colab\n",
        "        IN_COLAB = True\n",
        "    except:\n",
        "        IN_COLAB = False\n",
        "\n",
        "    if IN_COLAB:\n",
        "        # In Colab, run the demo directly\n",
        "        demo_brain_tumor_classifier()\n",
        "    else:\n",
        "        # Outside of Colab, use argparse\n",
        "        import argparse\n",
        "\n",
        "        parser = argparse.ArgumentParser(description=\"🧠 Brain Tumor Classification System\")\n",
        "        parser.add_argument('--mode', choices=['train', 'predict', 'demo', 'streamlit', 'gradio'],\n",
        "                           default='demo', help='Operation mode')\n",
        "        parser.add_argument('--training_data', type=str, help='Path to training data directory')\n",
        "        parser.add_argument('--image', type=str, help='Path to image for prediction')\n",
        "        parser.add_argument('--output_dir', type=str, default='./reports', help='Output directory for reports')\n",
        "\n",
        "        args = parser.parse_args()\n",
        "\n",
        "        if args.mode == 'demo':\n",
        "            demo_brain_tumor_classifier()\n",
        "\n",
        "        elif args.mode == 'train':\n",
        "            if not args.training_data:\n",
        "                print(\"❌ Training data path required for training mode\")\n",
        "                return\n",
        "\n",
        "            classifier = BrainTumorClassifier()\n",
        "            classifier.train_model(args.training_data)\n",
        "            print(\"✅ Model training completed!\")\n",
        "\n",
        "        elif args.mode == 'predict':\n",
        "            if not args.training_data or not args.image:\n",
        "                print(\"❌ Both training data path and image path required for prediction mode\")\n",
        "                return\n",
        "\n",
        "            classifier = BrainTumorClassifier()\n",
        "            classifier.train_model(args.training_data)\n",
        "\n",
        "            result = classifier.predict_single_image(args.image)\n",
        "            classifier.display_prediction_summary(result)\n",
        "\n",
        "            # Generate reports\n",
        "            md_path, pdf_path = classifier.generate_reports(result, args.output_dir)\n",
        "            print(f\"\\n📄 Reports saved:\")\n",
        "            print(f\"   • Markdown: {md_path}\")\n",
        "            print(f\"   • PDF: {pdf_path}\")\n",
        "\n",
        "        elif args.mode == 'streamlit':\n",
        "            print(\"🚀 Starting Streamlit app...\")\n",
        "            print(\"Run: streamlit run this_script.py --mode streamlit\")\n",
        "            create_streamlit_app()\n",
        "\n",
        "        elif args.mode == 'gradio':\n",
        "            print(\"🚀 Starting Gradio app...\")\n",
        "            app = create_gradio_app()\n",
        "            if app:\n",
        "                app.launch(share=True)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7cb423c139014a098ef3a29e8b0f075b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9300a6ea3bcd4f9dac598133bad5b799",
              "IPY_MODEL_4a1ac1a2f9d54969af0b5b48d0c96b88",
              "IPY_MODEL_b222d729f0c9442ea2030b10918730d2"
            ],
            "layout": "IPY_MODEL_b61d822665b341abb8d17256844bc269"
          }
        },
        "9300a6ea3bcd4f9dac598133bad5b799": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_251f6dbda51f4ee18ba339f63dc56af3",
            "placeholder": "​",
            "style": "IPY_MODEL_dc019972e2c64dc88b0acc2b80c0fc57",
            "value": "model.safetensors: 100%"
          }
        },
        "4a1ac1a2f9d54969af0b5b48d0c96b88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e024dcd0a7e342c7ac9147bc5dbfd72d",
            "max": 346284714,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_07904c0240974f51abda600cab916248",
            "value": 346284714
          }
        },
        "b222d729f0c9442ea2030b10918730d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6dfeedc0cf1b4b24a7e9a5d21a5d0132",
            "placeholder": "​",
            "style": "IPY_MODEL_5f22402fd2214d99b4b77b1d07126f9a",
            "value": " 346M/346M [00:03&lt;00:00, 109MB/s]"
          }
        },
        "b61d822665b341abb8d17256844bc269": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "251f6dbda51f4ee18ba339f63dc56af3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc019972e2c64dc88b0acc2b80c0fc57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e024dcd0a7e342c7ac9147bc5dbfd72d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07904c0240974f51abda600cab916248": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6dfeedc0cf1b4b24a7e9a5d21a5d0132": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f22402fd2214d99b4b77b1d07126f9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "220b3ae94b9d4219bdffd14a6b223f8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b615d48c938d4da38b27c4f7f8531b79",
              "IPY_MODEL_c3a70517191b4e5b97982faa539e1073",
              "IPY_MODEL_769401a13b3d44cd876865229a8eb015"
            ],
            "layout": "IPY_MODEL_d897b78520b74505b940ec98e0533e05"
          }
        },
        "b615d48c938d4da38b27c4f7f8531b79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ecba93d6e62b49d5a075742fe2d1073b",
            "placeholder": "​",
            "style": "IPY_MODEL_e16268bee90e4cb7b1cdbbcf7a628644",
            "value": "Best trial: 24. Best value: 0.897638: 100%"
          }
        },
        "c3a70517191b4e5b97982faa539e1073": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85e54948d49948bcbb35a9ae6c805052",
            "max": 30,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9ae0953ff23b43b8bf015ccfb65f4174",
            "value": 30
          }
        },
        "769401a13b3d44cd876865229a8eb015": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99ac06a05a9147ad852f774674403f40",
            "placeholder": "​",
            "style": "IPY_MODEL_bdf84749d1b94b589b1e676aa50c2de2",
            "value": " 30/30 [20:13&lt;00:00, 62.61s/it]"
          }
        },
        "d897b78520b74505b940ec98e0533e05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ecba93d6e62b49d5a075742fe2d1073b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e16268bee90e4cb7b1cdbbcf7a628644": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "85e54948d49948bcbb35a9ae6c805052": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ae0953ff23b43b8bf015ccfb65f4174": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "99ac06a05a9147ad852f774674403f40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bdf84749d1b94b589b1e676aa50c2de2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}